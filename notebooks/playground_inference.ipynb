{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = \"/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS\"\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from enum import Enum\n",
    "import albumentations as A\n",
    "from utils import AlbumentationsToTorchTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from utils import dotdictify\n",
    "from Transformer_SSL.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S1Bands(Enum):\n",
    "    VV = 1\n",
    "    VH = 2\n",
    "    ALL = [VV, VH]\n",
    "    NONE = None\n",
    "\n",
    "class Sensor(Enum):\n",
    "    s1 = \"s1\"\n",
    "    s2 = \"s2\"\n",
    "    lc = \"lc\"\n",
    "    dfc = \"dfc\"\n",
    "\n",
    "class S2Bands(Enum):\n",
    "    B01 = aerosol = 1\n",
    "    B02 = blue = 2\n",
    "    B03 = green = 3\n",
    "    B04 = red = 4\n",
    "    B05 = re1 = 5\n",
    "    B06 = re2 = 6\n",
    "    B07 = re3 = 7\n",
    "    B08 = nir1 = 8\n",
    "    B08A = nir2 = 9\n",
    "    B09 = vapor = 10\n",
    "    B10 = cirrus = 11\n",
    "    B11 = swir1 = 12\n",
    "    B12 = swir2 = 13\n",
    "    ALL = [B01, B02, B03, B04, B05, B06, B07, B08, B08A, B09, B10, B11, B12]\n",
    "    RGB = [B04, B03, B02]\n",
    "    NONE = None\n",
    "\n",
    "class LCBands(Enum):\n",
    "    LC = lc = 0\n",
    "    DFC = dfc = 1\n",
    "    ALL = [DFC]\n",
    "    NONE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(patch_path, bands, window=None):\n",
    "    \"\"\"\n",
    "        Returns raster data and image bounds for the defined bands of a specific patch\n",
    "        This method only loads a sinlge patch from a single sensor as defined by the bands specified\n",
    "    \"\"\"\n",
    "    # season = Seasons(season).value\n",
    "    sensor = None\n",
    "\n",
    "    if not bands:\n",
    "        return None, None\n",
    "\n",
    "    if isinstance(bands, (list, tuple)):\n",
    "        b = bands[0]\n",
    "    else:\n",
    "        b = bands\n",
    "    \n",
    "    if isinstance(b, S1Bands):\n",
    "        sensor = Sensor.s1.value\n",
    "        bandEnum = S1Bands\n",
    "    elif isinstance(b, S2Bands):\n",
    "        sensor = Sensor.s2.value\n",
    "        bandEnum = S2Bands\n",
    "    elif isinstance(b, LCBands):\n",
    "        if LCBands(bands) == LCBands.LC:\n",
    "            sensor = Sensor.lc.value \n",
    "        else:\n",
    "            sensor = Sensor.dfc.value \n",
    "\n",
    "        bands = LCBands(1)\n",
    "        bandEnum = LCBands\n",
    "    else:\n",
    "        raise Exception(\"Invalid bands specified\")\n",
    "\n",
    "    if isinstance(bands, (list, tuple)):\n",
    "        bands = [b.value for b in bands]\n",
    "    else:\n",
    "        bands = bandEnum(bands).value\n",
    "\n",
    "    with rasterio.open(patch_path) as patch:\n",
    "        if window is not None:\n",
    "            data = patch.read(bands, window=window) \n",
    "        else:\n",
    "            data = patch.read(bands)\n",
    "        bounds = patch.bounds\n",
    "\n",
    "    # Remap IGBP to DFC bands\n",
    "    # if sensor  == \"lc\":\n",
    "    #     data = IGBP2DFC[data]\n",
    "\n",
    "    if len(data.shape) == 2:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    return data, bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleSwinTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder1, encoder2, out_dim, device, freeze_layers=True):\n",
    "        super(DoubleSwinTransformerClassifier, self).__init__()\n",
    "        \n",
    "        # If you're only using one of the two backbones, just comment the one you don't need\n",
    "        self.backbone1 = encoder1\n",
    "        self.backbone2 = encoder2\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(\n",
    "            self.backbone2.num_features + self.backbone1.num_features,\n",
    "            out_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # freeze all layers but the last fc\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        x2, _, _ = self.backbone2.forward_features(x[\"s2\"].to(self.device))\n",
    "\n",
    "        z = torch.cat([x1, x2], dim=1)\n",
    "        z = self.fc(z)\n",
    "        \n",
    "        # If you're only using one of the two backbones, you may comment the lines above and use the following:\n",
    "        # x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        # z = self.fc(x1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFC_map_clean = {\n",
    "    0: \"Forest\",\n",
    "    1: \"Shrubland\",\n",
    "    2: \"Grassland\",\n",
    "    3: \"Wetlands\",\n",
    "    4: \"Croplands\",\n",
    "    5: \"Urban/Built-up\",\n",
    "    6: \"Barren\",\n",
    "    7: \"Water\",\n",
    "    255: \"Invalid\",\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': '../data/data_disini', # path to the training directory,  \n",
    "    'val_dir': '../data/data_disini', # path to the validation directory,\n",
    "    'train_mode': 'validation', # can be one of the following: 'test', 'validation'\n",
    "    'val_mode': 'test', # can be one of the following: 'test', 'validation'\n",
    "    'num_classes': 8, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': True, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5, \n",
    "    'target': 'dfc_label'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1076\n"
     ]
    }
   ],
   "source": [
    "image_px_size = 224\n",
    "if image_px_size != 256:\n",
    "    # crop the data to image_px_size times image_px_size (e.g. 128x128)\n",
    "    x_offset, y_offset = np.random.randint(0, 256 - image_px_size, 2)\n",
    "    window = Window(x_offset, y_offset, image_px_size, image_px_size)\n",
    "else:\n",
    "    window = None\n",
    "\n",
    "s1, bounds1 = get_patch(patch_path=\"../data/data_disini/ROIs0000_test/s1_0/ROIs0000_test_s1_0_p4548.tif\", bands=S1Bands.ALL, window=window)\n",
    "# s1, bounds1 = get_patch(patch_path=\"../data/inference/s1_combined.tif\", bands=S1Bands.ALL, window=window)\n",
    "s1, bounds1 = get_patch(patch_path=\"../data/inference/s1_vv.tif\", bands=S1Bands.VV, window=window)\n",
    "s2, bounds2 = get_patch(patch_path=\"../data/data_disini/ROIs0000_test/s2_0/ROIs0000_test_s2_0_p4548.tif\", bands=S2Bands.ALL, window=window)\n",
    "\n",
    "print(s1.min(), s1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_sample_values = True\n",
    "if clip_sample_values:\n",
    "    s1 = np.clip(s1, a_min=-25, a_max=0)\n",
    "    s1 = (\n",
    "        s1 + 25\n",
    "    )  # go from [-25,0] to [0,25] interval to make normalization easier\n",
    "    s2 = np.clip(s2, a_min=0, a_max=1e4)\n",
    "\n",
    "base_aug = A.Compose([ToTensorV2()])\n",
    "base_transform = AlbumentationsToTorchTransform(base_aug)\n",
    "\n",
    "s1 = base_transform(np.moveaxis(s1, 0, -1))\n",
    "s2 = base_transform(np.moveaxis(s2, 0, -1))\n",
    "\n",
    "s1_maxs = []\n",
    "for ch_idx in range(s1.shape[0]):\n",
    "    s1_maxs.append(\n",
    "        torch.ones((s1.shape[-2], s1.shape[-1])) * s1[ch_idx].max().item()\n",
    "        + 1e-5\n",
    "    )\n",
    "s1_maxs = torch.stack(s1_maxs)\n",
    "s2_maxs = []\n",
    "for b_idx in range(s2.shape[0]):\n",
    "    s2_maxs.append(\n",
    "        torch.ones((s2.shape[-2], s2.shape[-1])) * s2[b_idx].max().item() + 1e-5\n",
    "    )\n",
    "s2_maxs = torch.stack(s2_maxs)\n",
    "normalize = True\n",
    "if normalize:\n",
    "    s1 = s1 / s1_maxs\n",
    "    s2 = s2 / s2_maxs\n",
    "\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfc_dataset import DFCDataset\n",
    "\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/classifier-epoch-4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model \u001b[39m=\u001b[39m DoubleSwinTransformerClassifier(s1_backbone, s2_backbone, out_dim\u001b[39m=\u001b[39mdata_config[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../checkpoints/classifier-epoch-4.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m img \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39ms1\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39munsqueeze(s1, \u001b[39m0\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39ms2\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39munsqueeze(s2, \u001b[39m0\u001b[39m)} \u001b[39m# adding an extra dimension for batch information\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/playground_inference.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../checkpoints/classifier-epoch-4.pth'"
     ]
    }
   ],
   "source": [
    "accelerator = 'cpu'\n",
    "checkpoint = torch.load(\"../checkpoints/swin_t.pth\", map_location=torch.device(accelerator))\n",
    "weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "s1_weights = {k[len(\"backbone1.\"):]: v for k, v in weights.items() if \"backbone1\" in k}\n",
    "s2_weights = {k[len(\"backbone2.\"):]: v for k, v in weights.items() if \"backbone2\" in k}\n",
    "\n",
    "input_channels = train_config['s1_input_channels'] + train_config['s2_input_channels']\n",
    "\n",
    "with open(\"../configs/backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "\n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "swin_conf.model_config.MODEL.SWIN.IN_CHANS = 13\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "s1_backbone.load_state_dict(s1_weights)\n",
    "s2_backbone.load_state_dict(s2_weights)\n",
    "\n",
    "\n",
    "device = torch.device(accelerator)\n",
    "model = DoubleSwinTransformerClassifier(s1_backbone, s2_backbone, out_dim=data_config['num_classes'], device=device)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"../checkpoints/classifier-epoch-4.pth\"))\n",
    "\n",
    "\n",
    "img = {\"s1\": torch.unsqueeze(s1, 0), \"s2\": torch.unsqueeze(s2, 0)} # adding an extra dimension for batch information\n",
    "model.eval()\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_disini/ROIs0000_test/s1_0/ROIs0000_test_s1_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/s2_0/ROIs0000_test_s2_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/lc_0/ROIs0000_test_lc_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/dfc_0/ROIs0000_test_dfc_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/s1_0/ROIs0000_test_s1_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/s2_0/ROIs0000_test_s2_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/lc_0/ROIs0000_test_lc_0_p4548.tif\n",
      "data/data_disini/ROIs0000_test/dfc_0/ROIs0000_test_dfc_0_p4548.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7967, 0.7967, 0.7967,  ..., 0.8181, 0.8181, 0.8181],\n",
       "          [0.7967, 0.7967, 0.7967,  ..., 0.8181, 0.8181, 0.8181],\n",
       "          [0.7967, 0.7967, 0.7967,  ..., 0.8181, 0.8181, 0.8181],\n",
       "          ...,\n",
       "          [0.8388, 0.8388, 0.8388,  ..., 0.8213, 0.8213, 0.8213],\n",
       "          [0.8388, 0.8388, 0.8388,  ..., 0.8173, 0.8173, 0.8173],\n",
       "          [0.8388, 0.8388, 0.8388,  ..., 0.8173, 0.8173, 0.8173]],\n",
       "\n",
       "         [[0.4442, 0.4442, 0.4542,  ..., 0.4802, 0.4833, 0.4796],\n",
       "          [0.4641, 0.4641, 0.4442,  ..., 0.4665, 0.4907, 0.4777],\n",
       "          [0.4492, 0.4492, 0.4591,  ..., 0.4734, 0.4734, 0.4814],\n",
       "          ...,\n",
       "          [0.4994, 0.5056, 0.4994,  ..., 0.4703, 0.4572, 0.4597],\n",
       "          [0.4876, 0.4988, 0.5025,  ..., 0.4783, 0.4647, 0.4603],\n",
       "          [0.4808, 0.4845, 0.4839,  ..., 0.4684, 0.4690, 0.4641]],\n",
       "\n",
       "         [[0.2981, 0.2981, 0.3189,  ..., 0.3255, 0.3489, 0.3408],\n",
       "          [0.3052, 0.3052, 0.3144,  ..., 0.3205, 0.3489, 0.3316],\n",
       "          [0.3073, 0.3073, 0.3195,  ..., 0.3291, 0.3271, 0.3301],\n",
       "          ...,\n",
       "          [0.3911, 0.3834, 0.3880,  ..., 0.3286, 0.3118, 0.3266],\n",
       "          [0.3647, 0.3819, 0.3601,  ..., 0.3311, 0.3195, 0.3215],\n",
       "          [0.3443, 0.3687, 0.3520,  ..., 0.3306, 0.3321, 0.3195]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.7000, 0.7000, 0.7000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "          [0.7000, 0.7000, 0.7000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "          [0.7000, 0.7000, 0.7000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "          ...,\n",
       "          [0.6000, 0.6000, 0.6000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "          [0.6000, 0.6000, 0.6000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "          [0.6000, 0.6000, 0.6000,  ..., 0.5000, 0.5000, 0.5000]],\n",
       "\n",
       "         [[0.2691, 0.2691, 0.2691,  ..., 0.3076, 0.3115, 0.3115],\n",
       "          [0.2943, 0.2943, 0.2943,  ..., 0.2965, 0.2968, 0.2968],\n",
       "          [0.2943, 0.2943, 0.2943,  ..., 0.2965, 0.2968, 0.2968],\n",
       "          ...,\n",
       "          [0.4561, 0.4561, 0.4503,  ..., 0.2723, 0.2385, 0.2385],\n",
       "          [0.4561, 0.4561, 0.4503,  ..., 0.2519, 0.2341, 0.2341],\n",
       "          [0.4156, 0.4156, 0.4131,  ..., 0.2519, 0.2341, 0.2341]],\n",
       "\n",
       "         [[0.1264, 0.1264, 0.1264,  ..., 0.1864, 0.1855, 0.1855],\n",
       "          [0.1390, 0.1390, 0.1390,  ..., 0.1666, 0.1690, 0.1690],\n",
       "          [0.1390, 0.1390, 0.1390,  ..., 0.1666, 0.1690, 0.1690],\n",
       "          ...,\n",
       "          [0.2915, 0.2915, 0.2881,  ..., 0.1448, 0.1254, 0.1254],\n",
       "          [0.2915, 0.2915, 0.2881,  ..., 0.1303, 0.1288, 0.1288],\n",
       "          [0.2460, 0.2460, 0.2455,  ..., 0.1303, 0.1288, 0.1288]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_index = 13\n",
    "img_dataset = {\"s1\": torch.unsqueeze(val_dataset[img_index]['s1'], 0), \"s2\": torch.unsqueeze(val_dataset[img_index]['s2'], 0)} # adding an extra dimension for batch information\n",
    "img_dataset['s2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dataset['s2'] == s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset['s2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img['s2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from enum import Enum\n",
    "import albumentations as A\n",
    "from utils import AlbumentationsToTorchTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from utils import dotdictify\n",
    "from Transformer_SSL.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfc_dataset import DFCDataset\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': 'data/data_disini', # path to the training directory,  \n",
    "    'val_dir': 'data/data_disini', # path to the validation directory,\n",
    "    'train_mode': 'validation', # can be one of the following: 'test', 'validation'\n",
    "    'val_mode': 'test', # can be one of the following: 'test', 'validation'\n",
    "    'num_classes': 8, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
