{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from dfc_dataset import DFCDataset\n",
    "from metrics import ClasswiseAccuracy\n",
    "\n",
    "\n",
    "class DoubleResNetSimCLRDownstream(torch.nn.Module):\n",
    "    \"\"\"concatenate outputs from two backbones and add one linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(DoubleResNetSimCLRDownstream, self).__init__()\n",
    "\n",
    "        self.resnet_dict = {\"resnet18\": models.resnet18,\n",
    "                            \"resnet50\": models.resnet50,}\n",
    "        \n",
    "\n",
    "        self.backbone2 = self.resnet_dict.get(base_model)(pretrained=False, num_classes=out_dim)\n",
    "        dim_mlp2 = self.backbone2.fc.in_features\n",
    "        \n",
    "        # If you are using multimodal data you can un-comment the following lines:\n",
    "        # self.backbone1 = self.resnet_dict.get(base_model)(pretrained=False, num_classes=out_dim)\n",
    "        # dim_mlp1 = self.backbone1.fc.in_features\n",
    "        \n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(dim_mlp2, out_dim, bias=True)\n",
    "        # self.fc = torch.nn.Linear(dim_mlp1 + dim_mlp2, out_dim, bias=True)\n",
    "\n",
    "        # self.backbone1.fc = torch.nn.Identity()\n",
    "        self.backbone2.fc = torch.nn.Identity()\n",
    "\n",
    "    def _get_basemodel(self, model_name):\n",
    "        try:\n",
    "            model = self.resnet_dict[model_name]\n",
    "        except KeyError:\n",
    "            raise InvalidBackboneError(\n",
    "                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.backbone2(x[\"s2\"])\n",
    "\n",
    "        # If you are using multimodal data you can un-comment the following lines and comment z = self.fc(x2):\n",
    "        # x1 = self.backbone1(x[\"s1\"])\n",
    "        # z = torch.cat([x1, x2], dim=1)\n",
    "        # z = self.fc(z)\n",
    "     \n",
    "        z = self.fc(x2)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def load_trained_state_dict(self, weights):\n",
    "        \"\"\"load the pre-trained backbone weights\"\"\"\n",
    "        \n",
    "        # remove the MLP projection heads\n",
    "        for k in list(weights.keys()):\n",
    "            if k.startswith(('backbone1.fc', 'backbone2.fc')):\n",
    "                del weights[k]\n",
    "        \n",
    "        log = self.load_state_dict(weights, strict=False)\n",
    "        assert log.missing_keys == ['fc.weight', 'fc.bias']\n",
    "        \n",
    "        # freeze all layers but the last fc\n",
    "        for name, param in self.named_parameters():\n",
    "            if name not in ['fc.weight', 'fc.bias']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': 'data/data_disini',\n",
    "    'val_dir': 'data/data_disini',\n",
    "    'train_mode': 'test', # 'test', 'validation'\n",
    "    'val_mode': 'validation', # 'test', 'validation'\n",
    "    'num_classes': 8, # kepake\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224,\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': True, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4, # dipake\n",
    "    'batch_size': 16, # dipake\n",
    "    'epochs': 5, # diapke\n",
    "    'target': 'dfc_label' # dipake\n",
    "}\n",
    "\n",
    "train_dataset = DFCDataset(\n",
    "    data_config['train_dir'],\n",
    "    mode=data_config['train_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['train_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_train'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "base_model = \"resnet18\"\n",
    "num_classes = 8\n",
    "model = eval('DoubleResNetSimCLRDownstream')(base_model, num_classes)\n",
    "\n",
    "model.backbone2.conv1 = torch.nn.Conv2d(\n",
    "    train_config['s2_input_channels'],\n",
    "    64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(2, 2),\n",
    "    padding=(3, 3),\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "checkpoint = torch.load(\"checkpoints/resnet18.pth\", map_location=torch.device('mps'))\n",
    "model.load_trained_state_dict(checkpoint[\"state_dict\"])\n",
    "model = model.to(device)\n",
    "\n",
    "### Training ### \n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset[3]['s2'].shape) == [13, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(train_config['epochs']):\n",
    "    # Model Training\n",
    "    model.train()\n",
    "    step += 1\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # track performance\n",
    "    epoch_losses = torch.Tensor()\n",
    "    metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "    for idx, sample in enumerate(pbar):\n",
    "\n",
    "        if \"x\" in sample.keys():\n",
    "            if torch.isnan(sample[\"x\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        else:\n",
    "            if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "\n",
    "        # load input\n",
    "        s2 = sample[\"s2\"].to(device)\n",
    "        img = {\"s2\": s2}\n",
    "        \n",
    "        # if you are using a unimodal dataset (s1 for example), you may un-comment the following lines:\n",
    "        # s1 = sample[\"s1\"].to(device)\n",
    "        # img = {\"s1\": s1, \"s2\": s2}\n",
    "        \n",
    "        # load target\n",
    "        y = sample[train_config['target']].long().to(device)\n",
    "        \n",
    "        # model output\n",
    "        y_hat = model(img)\n",
    "        \n",
    "        # loss computation\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get prediction \n",
    "        _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "        epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "        metrics.add_batch(y, pred)\n",
    "\n",
    "        pbar.set_description(f\"Epoch:{epoch}, Training Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "    mean_loss = epoch_losses.mean()\n",
    "\n",
    "    train_stats = {\n",
    "            \"train_loss\": mean_loss.item(),\n",
    "            \"train_average_accuracy\": metrics.get_average_accuracy(),\n",
    "            \"train_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "            **{\n",
    "                \"train_accuracy_\" + k: v\n",
    "                for k, v in metrics.get_classwise_accuracy().items()\n",
    "            },\n",
    "        }\n",
    "    print(train_stats)\n",
    "\n",
    "    if epoch % 2 == 0:  \n",
    "\n",
    "        # Model Validation\n",
    "        model.eval()\n",
    "        pbar = tqdm(val_loader)\n",
    "\n",
    "        # track performance\n",
    "        epoch_losses = torch.Tensor()\n",
    "        metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(pbar):\n",
    "                if \"x\" in sample.keys():\n",
    "                    if torch.isnan(sample[\"x\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                else:\n",
    "                    if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                # load input\n",
    "                s2 = sample[\"s2\"].to(device)\n",
    "                img = {\"s2\": s2}\n",
    "\n",
    "                # if you are using a unimodal dataset (s1 for example), you may un-comment the following lines:\n",
    "                # s1 = sample[\"s1\"].to(device)\n",
    "                # img = {\"s1\": s1, \"s2\": s2}\n",
    "\n",
    "                # load target\n",
    "                y = sample[train_config['target']].long().to(device)\n",
    "\n",
    "                # model output\n",
    "                y_hat = model(img)\n",
    "\n",
    "                # loss computation\n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                # get prediction \n",
    "                _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "                epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "                metrics.add_batch(y, pred)\n",
    "\n",
    "\n",
    "                pbar.set_description(f\"Validation Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "            mean_loss = epoch_losses.mean()\n",
    "\n",
    "            val_stats = {\n",
    "                \"validation_loss\": mean_loss.item(),\n",
    "                \"validation_average_accuracy\": metrics.get_average_accuracy(),\n",
    "                \"validation_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "                **{\n",
    "                    \"validation_accuracy_\" + k: v\n",
    "                    for k, v in metrics.get_classwise_accuracy().items()\n",
    "                },\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch:{epoch}\", val_stats)\n",
    "            \n",
    "            # Save model checkpoint every 2 epochs \n",
    "            if epoch % 2 == 0:\n",
    "                if epoch == 0:\n",
    "                    continue\n",
    "\n",
    "                save_weights_path = (\n",
    "                    \"checkpoints/\" + \"-\".join([\"classifier\", \"epoch\", str(epoch)]) + \".pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), save_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
