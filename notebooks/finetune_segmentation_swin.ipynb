{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen backbone\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = '/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS'\n",
    "os.chdir(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from dfc_dataset import DFCDataset\n",
    "from Transformer_SSL.models import build_model\n",
    "from Transformer_SSL.models.swin_transformer import DoubleSwinTransformerSegmentation\n",
    "from utils import dotdictify\n",
    "from metrics import ClasswiseAccuracy\n",
    "\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': False, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5, \n",
    "    'target': 'dfc_label'\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': 'data/data_disini', # path to the training directory,  \n",
    "    'val_dir': 'data/data_disini', # path to the validation directory,\n",
    "    'train_mode': 'validation', # can be one of the following: 'test', 'validation'\n",
    "    'val_mode': 'test', # can be one of the following: 'test', 'validation'\n",
    "    'num_classes': 8, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "train_dataset = DFCDataset(\n",
    "    data_config['train_dir'],\n",
    "    mode=data_config['train_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['train_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_train'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "### Load Pretrain ###\n",
    "checkpoint = torch.load(\"checkpoints/swin_t.pth\", map_location=torch.device('cpu')) \n",
    "weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "s1_weights = {k[len(\"backbone1.\"):]: v for k, v in weights.items() if \"backbone1\" in k}\n",
    "s2_weights = {k[len(\"backbone2.\"):]: v for k, v in weights.items() if \"backbone2\" in k}\n",
    "\n",
    "with open(\"configs/backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "\n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "swin_conf.model_config.MODEL.SWIN.IN_CHANS = 13\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "s1_backbone.load_state_dict(s1_weights)\n",
    "s2_backbone.load_state_dict(s2_weights)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = DoubleSwinTransformerSegmentation(\n",
    "        s1_backbone, s2_backbone, out_dim=data_config['num_classes'], device=device\n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "### Training ###\n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10ed06d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 0/62 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# loss computation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_hat, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# backward step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/notebooks/finetune_segmentation_swin.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "\n",
    "step = 0\n",
    "for epoch in range(train_config['epochs']):\n",
    "    # Model Training\n",
    "    model.train()\n",
    "    step += 1\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # track performance\n",
    "    epoch_losses = torch.Tensor()\n",
    "    metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "    for idx, sample in enumerate(pbar):\n",
    "\n",
    "        if \"x\" in sample.keys():\n",
    "            if torch.isnan(sample[\"x\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        else:\n",
    "            if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        \n",
    "        # load input\n",
    "        s1 = sample[\"s1\"].to(device)\n",
    "        s2 = sample[\"s2\"].to(device)\n",
    "        img = {\"s1\": s1, \"s2\": s2}\n",
    "        \n",
    "        # if you are using a unimodal dataset (s1 for example), you may comment the lines above and use the following:\n",
    "        # s1 = sample[\"s1\"].to(device)\n",
    "        # img = {\"s1\": s1}\n",
    "        \n",
    "        # load target\n",
    "        y = sample[train_config['target']].long().to(device)\n",
    "        \n",
    "        # model output\n",
    "        y_hat = model(img)\n",
    "        \n",
    "        # loss computation\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get prediction \n",
    "        _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "        epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "        metrics.add_batch(y, pred)\n",
    "\n",
    "        pbar.set_description(f\"Epoch:{epoch}, Training Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "    mean_loss = epoch_losses.mean()\n",
    "\n",
    "    train_stats = {\n",
    "            \"train_loss\": mean_loss.item(),\n",
    "            \"train_average_accuracy\": metrics.get_average_accuracy(),\n",
    "            \"train_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "            **{\n",
    "                \"train_accuracy_\" + k: v\n",
    "                for k, v in metrics.get_classwise_accuracy().items()\n",
    "            },\n",
    "        }\n",
    "    print(train_stats)\n",
    "\n",
    "    if epoch % 2 == 0:  \n",
    "\n",
    "        # Model Validation\n",
    "        model.eval()\n",
    "        pbar = tqdm(val_loader)\n",
    "\n",
    "        # track performance\n",
    "        epoch_losses = torch.Tensor()\n",
    "        metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(pbar):\n",
    "                if \"x\" in sample.keys():\n",
    "                    if torch.isnan(sample[\"x\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                else:\n",
    "                    if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                # load input\n",
    "                s1 = sample[\"s1\"].to(device)\n",
    "                s2 = sample[\"s2\"].to(device)\n",
    "                img = {\"s1\": s1, \"s2\": s2}\n",
    "\n",
    "                # if you are using a unimodal dataset (s1 for example), you may comment the lines above and use the following:\n",
    "                # s1 = sample[\"s1\"].to(device)\n",
    "                # img = {\"s1\": s1}\n",
    "\n",
    "                # load target\n",
    "                y = sample[train_config['target']].long().to(device)\n",
    "\n",
    "                # model output\n",
    "                y_hat = model(img)\n",
    "\n",
    "                # loss computation\n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                # get prediction \n",
    "                _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "                epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "                metrics.add_batch(y, pred)\n",
    "\n",
    "\n",
    "                pbar.set_description(f\"Validation Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "            mean_loss = epoch_losses.mean()\n",
    "\n",
    "            val_stats = {\n",
    "                \"validation_loss\": mean_loss.item(),\n",
    "                \"validation_average_accuracy\": metrics.get_average_accuracy(),\n",
    "                \"validation_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "                **{\n",
    "                    \"validation_accuracy_\" + k: v\n",
    "                    for k, v in metrics.get_classwise_accuracy().items()\n",
    "                },\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch:{epoch}\", val_stats)\n",
    "            \n",
    "            # Save model checkpoint every 2 epochs \n",
    "            if epoch % 2 == 0:\n",
    "                if epoch == 0:\n",
    "                    continue\n",
    "\n",
    "                save_weights_path = (\n",
    "                    \"checkpoints/\" + \"-\".join([\"classifier\", \"epoch\", str(epoch)]) + \".pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), save_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "  3%|â–Ž         | 2/62 [00:17<08:31,  8.53s/it]"
     ]
    }
   ],
   "source": [
    "model.train() \n",
    "pbar = tqdm(train_loader)\n",
    "y_hats = {}\n",
    "for idx, sample in enumerate(pbar):\n",
    "\n",
    "    # if \"x\" in sample.keys():\n",
    "    #     if torch.isnan(sample[\"x\"]).any():\n",
    "    #         # some s1 scenes are known to have NaNs...\n",
    "    #         continue\n",
    "    # else:\n",
    "    #     if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "    #         # some s1 scenes are known to have NaNs...\n",
    "    #         continue\n",
    "    \n",
    "    # load input\n",
    "    s1 = sample[\"s1\"].to(device)\n",
    "    s2 = sample[\"s2\"].to(device)\n",
    "    img = {\"s1\": s1, \"s2\": s2}\n",
    "\n",
    "    y_hat = model(img)\n",
    "    y_hats[sample['idx']] = y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
