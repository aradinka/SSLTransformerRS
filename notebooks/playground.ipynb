{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "class S1Bands(Enum):\n",
    "    VV = 1\n",
    "    VH = 2\n",
    "    ALL = [VV, VH]\n",
    "    NONE = None\n",
    "\n",
    "class Sensor(Enum):\n",
    "    s1 = \"s1\"\n",
    "    s2 = \"s2\"\n",
    "    lc = \"lc\"\n",
    "    dfc = \"dfc\"\n",
    "\n",
    "class S2Bands(Enum):\n",
    "    B01 = aerosol = 1\n",
    "    B02 = blue = 2\n",
    "    B03 = green = 3\n",
    "    B04 = red = 4\n",
    "    B05 = re1 = 5\n",
    "    B06 = re2 = 6\n",
    "    B07 = re3 = 7\n",
    "    B08 = nir1 = 8\n",
    "    B08A = nir2 = 9\n",
    "    B09 = vapor = 10\n",
    "    B10 = cirrus = 11\n",
    "    B11 = swir1 = 12\n",
    "    B12 = swir2 = 13\n",
    "    ALL = [B01, B02, B03, B04, B05, B06, B07, B08, B08A, B09, B10, B11, B12]\n",
    "    RGB = [B04, B03, B02]\n",
    "    NONE = None\n",
    "\n",
    "class LCBands(Enum):\n",
    "    LC = lc = 0\n",
    "    DFC = dfc = 1\n",
    "    ALL = [DFC]\n",
    "    NONE = None\n",
    "\n",
    "def get_patch(patch_path, bands, window=None):\n",
    "    \"\"\"\n",
    "        Returns raster data and image bounds for the defined bands of a specific patch\n",
    "        This method only loads a sinlge patch from a single sensor as defined by the bands specified\n",
    "    \"\"\"\n",
    "    # season = Seasons(season).value\n",
    "    sensor = None\n",
    "\n",
    "    if not bands:\n",
    "        return None, None\n",
    "\n",
    "    if isinstance(bands, (list, tuple)):\n",
    "        b = bands[0]\n",
    "    else:\n",
    "        b = bands\n",
    "    \n",
    "    if isinstance(b, S1Bands):\n",
    "        sensor = Sensor.s1.value\n",
    "        bandEnum = S1Bands\n",
    "    elif isinstance(b, S2Bands):\n",
    "        sensor = Sensor.s2.value\n",
    "        bandEnum = S2Bands\n",
    "    elif isinstance(b, LCBands):\n",
    "        if LCBands(bands) == LCBands.LC:\n",
    "            sensor = Sensor.lc.value \n",
    "        else:\n",
    "            sensor = Sensor.dfc.value \n",
    "\n",
    "        bands = LCBands(1)\n",
    "        bandEnum = LCBands\n",
    "    else:\n",
    "        raise Exception(\"Invalid bands specified\")\n",
    "\n",
    "    if isinstance(bands, (list, tuple)):\n",
    "        bands = [b.value for b in bands]\n",
    "    else:\n",
    "        bands = bandEnum(bands).value\n",
    "\n",
    "    print(bands)\n",
    "    with rasterio.open(patch_path) as patch:\n",
    "        if window is not None:\n",
    "            data = patch.read(bands, window=window) \n",
    "        else:\n",
    "            data = patch.read(bands)\n",
    "        bounds = patch.bounds\n",
    "\n",
    "    # Remap IGBP to DFC bands\n",
    "    # if sensor  == \"lc\":\n",
    "    #     data = IGBP2DFC[data]\n",
    "\n",
    "    if len(data.shape) == 2:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    return data, bounds\n",
    "\n",
    "image_px_size = 224\n",
    "if image_px_size != 256:\n",
    "    # crop the data to image_px_size times image_px_size (e.g. 128x128)\n",
    "    x_offset, y_offset = np.random.randint(0, 256 - image_px_size, 2)\n",
    "    window = Window(x_offset, y_offset, image_px_size, image_px_size)\n",
    "else:\n",
    "    window = None\n",
    "\n",
    "s1, bounds1 = get_patch(patch_path=\"../data/inference/s1_combined.tif\", bands=S1Bands.ALL, window=window)\n",
    "s2, bounds1 = get_patch(patch_path=\"../data/inference/S2 Composite.tif\", bands=S2Bands.ALL, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9632, 9583)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_path = \"../data/inference/s1_combined.tif\"\n",
    "s1_bands = [1, 2]\n",
    "s2_bands = [i for i in range(1, 14)]\n",
    "\n",
    "image_px_size = 224\n",
    "if image_px_size != 256:\n",
    "    # crop the data to image_px_size times image_px_size (e.g. 128x128)\n",
    "    x_offset, y_offset = np.random.randint(0, 256 - image_px_size, 2)\n",
    "    window = Window(x_offset, y_offset, image_px_size, image_px_size)\n",
    "else:\n",
    "    window = None\n",
    "\n",
    "window = None\n",
    "\n",
    "bands = s1_bands\n",
    "with rasterio.open(patch_path) as patch:\n",
    "    if window is not None:\n",
    "        data = patch.read(bands, window=window) \n",
    "    else:\n",
    "        data = patch.read(bands)\n",
    "    bounds = patch.bounds\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10980, 10980)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_bands = [1, 2]\n",
    "s2_bands = [i for i in range(1, 14)]\n",
    "patch_path = \"../data/inference/S2 Composite.tif\"\n",
    "bands = s2_bands\n",
    "\n",
    "def load_tif(patch_path, bands):\n",
    "    with rasterio.open(patch_path) as patch:\n",
    "        data = patch.read(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(s1[0])\n",
    "axs[0].set_title(\"Sentinel-1 VV polarization\")\n",
    "axs[1].imshow(s1[1])\n",
    "axs[1].set_title(\"Sentinel-1 VH polarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "\n",
    "# Paths to your VV and VH images\n",
    "vv_image_path = \"../data/inference/s1_vv.tif\"\n",
    "vh_image_path = \"../data/inference/s1_vh.tif\"\n",
    "\n",
    "# Read the VV and VH images\n",
    "with rasterio.open(vv_image_path) as vv_src:\n",
    "    vv_data = vv_src.read(1)\n",
    "\n",
    "with rasterio.open(vh_image_path) as vh_src:\n",
    "    vh_data = vh_src.read(1)\n",
    "\n",
    "# Check that both images have the same shape and CRS\n",
    "assert vv_src.shape == vh_src.shape\n",
    "assert vv_src.crs == vh_src.crs\n",
    "\n",
    "# Create a new multi-band raster file\n",
    "output_path = \"../data/inference/s1_combined.tif\"\n",
    "with rasterio.open(\n",
    "    output_path, \n",
    "    'w', \n",
    "    driver='GTiff',\n",
    "    height=vv_src.height, \n",
    "    width=vv_src.width, \n",
    "    count=2,  # Number of bands\n",
    "    dtype=vv_data.dtype,\n",
    "    crs=vv_src.crs,\n",
    "    transform=vv_src.transform\n",
    ") as dst:\n",
    "    dst.write(vv_data, 1)  # Write VV data to band 1\n",
    "    dst.write(vh_data, 2)  # Write VH data to band 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleSwinTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder1, encoder2, out_dim, device, freeze_layers=True):\n",
    "        super(DoubleSwinTransformerClassifier, self).__init__()\n",
    "        \n",
    "        # If you're only using one of the two backbones, just comment the one you don't need\n",
    "        self.backbone1 = encoder1\n",
    "        self.backbone2 = encoder2\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(\n",
    "            self.backbone2.num_features + self.backbone1.num_features,\n",
    "            out_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # freeze all layers but the last fc\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        x2, _, _ = self.backbone2.forward_features(x[\"s2\"].to(self.device))\n",
    "\n",
    "        z = torch.cat([x1, x2], dim=1)\n",
    "        z = self.fc(z)\n",
    "        \n",
    "        # If you're only using one of the two backbones, you may comment the lines above and use the following:\n",
    "        # x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        # z = self.fc(x1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer_SSL.models import build_model\n",
    "from utils import dotdictify\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"configs/backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "# Data configurations:\n",
    "data_config = {\n",
    "    'train_dir': '/data/grss-dfc-20', # path to the training directory,  \n",
    "    'val_dir': '/data/grss-dfc-20', # path to the validation directory,\n",
    "    'train_mode': 'validation', # can be one of the following: 'test', 'validation'\n",
    "    'val_mode': 'test', # can be one of the following: 'test', 'validation'\n",
    "    'num_classes': 8, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the checkpoint\n",
    "checkpoint = torch.load(\n",
    "    \"checkpoints/swin_t.pth\",\n",
    "    map_location=torch.device('cpu')\n",
    ") \n",
    "weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Sentinel-1 stream weights\n",
    "s1_weights = {\n",
    "    k[len(\"backbone1.\"):]: v for k, v in weights.items() if \"backbone1\" in k\n",
    "}\n",
    "\n",
    "# Sentinel-2 stream weights\n",
    "s2_weights = {\n",
    "    k[len(\"backbone2.\"):]: v for k, v in weights.items() if \"backbone2\" in k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer_SSL.models.swin_transformer import DoubleSwinTransformerDownstream\n",
    "from utils import save_checkpoint_single_model, dotdictify\n",
    "from Transformer_SSL.models import build_model\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu:0\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# Training configurations\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': True, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5, \n",
    "    'target': 'dfc_label'\n",
    "}\n",
    "\n",
    "# Input channel size\n",
    "input_channels = train_config['s1_input_channels'] + train_config['s2_input_channels']\n",
    "\n",
    "# If you are using a uni-modal dataset, you can un-comment one of these lines, and comment the one above:\n",
    "# input_channels = train_config['s1_input_channels']\n",
    "# input_channels = train_config['s2_input_channels']\n",
    "\n",
    "with open(\"configs/backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "\n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "swin_conf.model_config.MODEL.SWIN.IN_CHANS = 13\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "s1_backbone.load_state_dict(s1_weights)\n",
    "s2_backbone.load_state_dict(s2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleSwinTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder1, encoder2, out_dim, device, freeze_layers=True):\n",
    "        super(DoubleSwinTransformerClassifier, self).__init__()\n",
    "        \n",
    "        # If you're only using one of the two backbones, just comment the one you don't need\n",
    "        self.backbone1 = encoder1\n",
    "        self.backbone2 = encoder2\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(\n",
    "            self.backbone2.num_features + self.backbone1.num_features,\n",
    "            out_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # freeze all layers but the last fc\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        x2, _, _ = self.backbone2.forward_features(x[\"s2\"].to(self.device))\n",
    "\n",
    "        z = torch.cat([x1, x2], dim=1)\n",
    "        z = self.fc(z)\n",
    "        \n",
    "        # If you're only using one of the two backbones, you may comment the lines above and use the following:\n",
    "        # x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        # z = self.fc(x1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DoubleSwinTransformerClassifier(\n",
    "        s1_backbone, s2_backbone, out_dim=data_config['num_classes'], device=device\n",
    "    )\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)\n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfc_dataset import DFCDataset\n",
    "\n",
    "# Create Training Dataset\n",
    "train_dataset = DFCDataset(\n",
    "    data_config['train_dir'],\n",
    "    mode=data_config['train_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['train_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_train'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "# Create Validation Dataset\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model's instance\n",
    "model = DoubleSwinTransformerClassifier(\n",
    "        s1_backbone, s2_backbone, out_dim=data_config['num_classes'], device=device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# load checkpoints weights\n",
    "model.load_state_dict(torch.load(\"checkpoints/classifier-epoch-4.pth\"))\n",
    "\n",
    "# prepare input and feed it to model for evaluation\n",
    "img = {\"s1\": torch.unsqueeze(val_dataset[2]['s1'], 0), \"s2\": torch.unsqueeze(val_dataset[2]['s2'], 0)} # adding an extra dimension for batch information\n",
    "model.eval()\n",
    "output = model(img)\n",
    "\n",
    "# display predicted class:\n",
    "print(f'Predicted class: {DFC_map_clean[torch.argmax(output).item()]}')\n",
    "\n",
    "# display ground-truth label:\n",
    "print('Ground-truth class: ', DFC_map_clean[val_dataset[2][train_config['target']]])\n",
    "\n",
    "# display image\n",
    "val_dataset.visualize_observation(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFC SEN12MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(\"data/dfc_sen12ms_dataset.py/Data_for_training_zoom_031123.shp\")\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
