{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Epoch:0, Training Loss:1.741:   5%|â–         | 3/62 [00:15<04:21,  4.43s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = '/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS'\n",
    "os.chdir(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from dfc_dataset import DFCDataset\n",
    "from Transformer_SSL.models import build_model\n",
    "from utils import dotdictify\n",
    "from metrics import ClasswiseAccuracy\n",
    "\n",
    "\n",
    "class DoubleSwinTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder1, encoder2, out_dim, device, freeze_layers=True):\n",
    "        super(DoubleSwinTransformerClassifier, self).__init__()\n",
    "        \n",
    "        # If you're only using one of the two backbones, just comment the one you don't need\n",
    "        self.backbone1 = encoder1\n",
    "        self.backbone2 = encoder2\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(\n",
    "            self.backbone2.num_features + self.backbone1.num_features,\n",
    "            out_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # freeze all layers but the last fc\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        x2, _, _ = self.backbone2.forward_features(x[\"s2\"].to(self.device))\n",
    "\n",
    "        z = torch.cat([x1, x2], dim=1)\n",
    "        z = self.fc(z)\n",
    "        \n",
    "        # If you're only using one of the two backbones, you may comment the lines above and use the following:\n",
    "        # x1, _, _ = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "        # z = self.fc(x1)\n",
    "\n",
    "        return z\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': True, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5, \n",
    "    'target': 'dfc_label'\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': 'data/data_disini', # path to the training directory,  \n",
    "    'val_dir': 'data/data_disini', # path to the validation directory,\n",
    "    'train_mode': 'validation', # can be one of the following: 'test', 'validation'\n",
    "    'val_mode': 'test', # can be one of the following: 'test', 'validation'\n",
    "    'num_classes': 8, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "train_dataset = DFCDataset(\n",
    "    data_config['train_dir'],\n",
    "    mode=data_config['train_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['train_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_train'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "### Load Pretrain ###\n",
    "checkpoint = torch.load(\"checkpoints/swin_t.pth\", map_location=torch.device('cpu')) \n",
    "weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "s1_weights = {k[len(\"backbone1.\"):]: v for k, v in weights.items() if \"backbone1\" in k}\n",
    "s2_weights = {k[len(\"backbone2.\"):]: v for k, v in weights.items() if \"backbone2\" in k}\n",
    "\n",
    "with open(\"configs/backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "\n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "swin_conf.model_config.MODEL.SWIN.IN_CHANS = 13\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "s1_backbone.load_state_dict(s1_weights)\n",
    "s2_backbone.load_state_dict(s2_weights)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = DoubleSwinTransformerClassifier(\n",
    "        s1_backbone, s2_backbone, out_dim=data_config['num_classes'], device=device\n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "### Training ###\n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(train_config['epochs']):\n",
    "    # Model Training\n",
    "    model.train()\n",
    "    step += 1\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # track performance\n",
    "    epoch_losses = torch.Tensor()\n",
    "    metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "    for idx, sample in enumerate(pbar):\n",
    "\n",
    "        if \"x\" in sample.keys():\n",
    "            if torch.isnan(sample[\"x\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        else:\n",
    "            if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        \n",
    "        # load input\n",
    "        s1 = sample[\"s1\"].to(device)\n",
    "        s2 = sample[\"s2\"].to(device)\n",
    "        img = {\"s1\": s1, \"s2\": s2}\n",
    "        \n",
    "        # if you are using a unimodal dataset (s1 for example), you may comment the lines above and use the following:\n",
    "        # s1 = sample[\"s1\"].to(device)\n",
    "        # img = {\"s1\": s1}\n",
    "        \n",
    "        # load target\n",
    "        y = sample[train_config['target']].long().to(device)\n",
    "        \n",
    "        # model output\n",
    "        y_hat = model(img)\n",
    "        \n",
    "        # loss computation\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get prediction \n",
    "        _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "        epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "        metrics.add_batch(y, pred)\n",
    "\n",
    "        pbar.set_description(f\"Epoch:{epoch}, Training Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "    mean_loss = epoch_losses.mean()\n",
    "\n",
    "    train_stats = {\n",
    "            \"train_loss\": mean_loss.item(),\n",
    "            \"train_average_accuracy\": metrics.get_average_accuracy(),\n",
    "            \"train_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "            **{\n",
    "                \"train_accuracy_\" + k: v\n",
    "                for k, v in metrics.get_classwise_accuracy().items()\n",
    "            },\n",
    "        }\n",
    "    print(train_stats)\n",
    "\n",
    "    if epoch % 2 == 0:  \n",
    "\n",
    "        # Model Validation\n",
    "        model.eval()\n",
    "        pbar = tqdm(val_loader)\n",
    "\n",
    "        # track performance\n",
    "        epoch_losses = torch.Tensor()\n",
    "        metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(pbar):\n",
    "                if \"x\" in sample.keys():\n",
    "                    if torch.isnan(sample[\"x\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                else:\n",
    "                    if torch.isnan(sample[\"s1\"]).any() or torch.isnan(sample[\"s2\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                # load input\n",
    "                s1 = sample[\"s1\"].to(device)\n",
    "                s2 = sample[\"s2\"].to(device)\n",
    "                img = {\"s1\": s1, \"s2\": s2}\n",
    "\n",
    "                # if you are using a unimodal dataset (s1 for example), you may comment the lines above and use the following:\n",
    "                # s1 = sample[\"s1\"].to(device)\n",
    "                # img = {\"s1\": s1}\n",
    "\n",
    "                # load target\n",
    "                y = sample[train_config['target']].long().to(device)\n",
    "\n",
    "                # model output\n",
    "                y_hat = model(img)\n",
    "\n",
    "                # loss computation\n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                # get prediction \n",
    "                _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "                epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "                metrics.add_batch(y, pred)\n",
    "\n",
    "\n",
    "                pbar.set_description(f\"Validation Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "            mean_loss = epoch_losses.mean()\n",
    "\n",
    "            val_stats = {\n",
    "                \"validation_loss\": mean_loss.item(),\n",
    "                \"validation_average_accuracy\": metrics.get_average_accuracy(),\n",
    "                \"validation_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "                **{\n",
    "                    \"validation_accuracy_\" + k: v\n",
    "                    for k, v in metrics.get_classwise_accuracy().items()\n",
    "                },\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch:{epoch}\", val_stats)\n",
    "            \n",
    "            # Save model checkpoint every 2 epochs \n",
    "            if epoch % 2 == 0:\n",
    "                if epoch == 0:\n",
    "                    continue\n",
    "\n",
    "                save_weights_path = (\n",
    "                    \"checkpoints/\" + \"-\".join([\"classifier\", \"epoch\", str(epoch)]) + \".pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), save_weights_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
