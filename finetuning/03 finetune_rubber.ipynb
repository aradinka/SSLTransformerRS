{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir('/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from dfc_dataset import DFCDataset\n",
    "from metrics import ClasswiseAccuracy\n",
    "\n",
    "\n",
    "class DoubleResNetSimCLRDownstream(torch.nn.Module):\n",
    "    \"\"\"concatenate outputs from two backbones and add one linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(DoubleResNetSimCLRDownstream, self).__init__()\n",
    "\n",
    "        self.resnet_dict = {\"resnet18\": models.resnet18,\n",
    "                            \"resnet50\": models.resnet50,}\n",
    "        \n",
    "\n",
    "        self.backbone2 = self.resnet_dict.get(base_model)(pretrained=False, num_classes=out_dim)\n",
    "        dim_mlp2 = self.backbone2.fc.in_features\n",
    "        \n",
    "        # If you are using multimodal data you can un-comment the following lines:\n",
    "        # self.backbone1 = self.resnet_dict.get(base_model)(pretrained=False, num_classes=out_dim)\n",
    "        # dim_mlp1 = self.backbone1.fc.in_features\n",
    "        \n",
    "        # add final linear layer\n",
    "        self.fc = torch.nn.Linear(dim_mlp2, out_dim, bias=True)\n",
    "        # self.fc = torch.nn.Linear(dim_mlp1 + dim_mlp2, out_dim, bias=True)\n",
    "\n",
    "        # self.backbone1.fc = torch.nn.Identity()\n",
    "        self.backbone2.fc = torch.nn.Identity()\n",
    "\n",
    "    def _get_basemodel(self, model_name):\n",
    "        try:\n",
    "            model = self.resnet_dict[model_name]\n",
    "        except KeyError:\n",
    "            raise InvalidBackboneError(\n",
    "                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.backbone2(x[\"s2\"])\n",
    "\n",
    "        # If you are using multimodal data you can un-comment the following lines and comment z = self.fc(x2):\n",
    "        # x1 = self.backbone1(x[\"s1\"])\n",
    "        # z = torch.cat([x1, x2], dim=1)\n",
    "        # z = self.fc(z)\n",
    "     \n",
    "        z = self.fc(x2)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def load_trained_state_dict(self, weights):\n",
    "        \"\"\"load the pre-trained backbone weights\"\"\"\n",
    "        \n",
    "        # remove the MLP projection heads\n",
    "        for k in list(weights.keys()):\n",
    "            if k.startswith(('backbone1.fc', 'backbone2.fc')):\n",
    "                del weights[k]\n",
    "        \n",
    "        log = self.load_state_dict(weights, strict=False)\n",
    "        assert log.missing_keys == ['fc.weight', 'fc.bias']\n",
    "        \n",
    "        # freeze all layers but the last fc\n",
    "        for name, param in self.named_parameters():\n",
    "            if name not in ['fc.weight', 'fc.bias']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "data_config = {\n",
    "    'train_dir': '../data/data_disini',\n",
    "    'val_dir': '../data/data_disini',\n",
    "    'train_mode': 'validation', # 'test', 'validation'\n",
    "    'val_mode': 'test', # 'test', 'validation'\n",
    "    'num_classes': 9, # kepake\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224,\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': True, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.00001,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4, # dipake\n",
    "    'batch_size': 16, # dipake\n",
    "    'epochs': 5, # diapke\n",
    "    'target': 'dfc_label' # dipake\n",
    "}\n",
    "\n",
    "train_dataset = DFCDataset(\n",
    "    data_config['train_dir'],\n",
    "    mode=data_config['train_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['train_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_train'],\n",
    "    seed=data_config['seed'],\n",
    "    \n",
    "    add_cacao=True,\n",
    ")\n",
    "val_dataset = DFCDataset(\n",
    "    data_config['val_dir'],\n",
    "    mode=data_config['val_mode'],\n",
    "    clip_sample_values=data_config['clip_sample_values'],\n",
    "    used_data_fraction=data_config['val_used_data_fraction'],\n",
    "    image_px_size=data_config['image_px_size'],\n",
    "    cover_all_parts=data_config['cover_all_parts_validation'],\n",
    "    seed=data_config['seed'],\n",
    "\n",
    "    add_cacao=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_config['dataloader_workers'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Validation ##\n",
      "Loader: 1053\n",
      "Original csv: 986\n",
      "New csv: 1053\n",
      "DFC: 1053\n",
      "S2: 1053\n",
      "\n",
      "## Test ##\n",
      "Loader: 5397\n",
      "Original csv: 5128\n",
      "New csv: 5397\n",
      "DFC: 5397\n",
      "S2: 5397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "val_obs = pd.read_csv(\"../data/data_disini/validation_observations.csv\", header=None, names=[\"Season\", \"Scene\", \"ID\"])\n",
    "test_obs = pd.read_csv(\"../data/data_disini/test_observations.csv\", header=None, names=[\"Season\", \"Scene\", \"ID\"])\n",
    "\n",
    "val_rubber = pd.read_csv(\"../data/data_disini/validation_observations_rubber.csv\", header=None, names=[\"Season\", \"Scene\", \"ID\"])\n",
    "test_rubber = pd.read_csv(\"../data/data_disini/test_observations_rubber.csv\", header=None, names=[\"Season\", \"Scene\", \"ID\"])\n",
    "\n",
    "val_dfc = [x for x in os.listdir(\"../data/data_disini/ROIs0000_validation/dfc_0\") if x.split(\".\")[-1] == \"tif\"]\n",
    "test_dfc = [x for x in os.listdir(\"../data/data_disini/ROIs0000_test/dfc_0\") if x.split(\".\")[-1] == \"tif\"]\n",
    "\n",
    "val_s2 = [x for x in os.listdir(\"../data/data_disini/ROIs0000_validation/s2_0\") if x.split(\".\")[-1] == \"tif\"]\n",
    "test_s2 = [x for x in os.listdir(\"../data/data_disini/ROIs0000_test/s2_0\") if x.split(\".\")[-1] == \"tif\"]\n",
    "\n",
    "p = f\"\"\"\n",
    "## Validation ##\n",
    "Loader: {len(train_dataset)}\n",
    "Original csv: {len(val_obs[\"ID\"].unique())}\n",
    "New csv: {len(val_rubber[\"ID\"].unique())}\n",
    "DFC: {len(val_dfc)}\n",
    "S2: {len(val_s2)}\n",
    "\n",
    "## Test ##\n",
    "Loader: {len(val_dataset)}\n",
    "Original csv: {test_obs.shape[0]}\n",
    "New csv: {test_rubber.shape[0]}\n",
    "DFC: {len(test_dfc)}\n",
    "S2: {len(test_s2)}\n",
    "\"\"\"\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning\n"
     ]
    }
   ],
   "source": [
    "base_model = \"resnet18\"\n",
    "num_classes = 9\n",
    "model = eval('DoubleResNetSimCLRDownstream')(base_model, num_classes)\n",
    "\n",
    "model.backbone2.conv1 = torch.nn.Conv2d(\n",
    "    train_config['s2_input_channels'],\n",
    "    64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(2, 2),\n",
    "    padding=(3, 3),\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "checkpoint = torch.load(\"../checkpoints/resnet18.pth\", map_location=torch.device('mps'))\n",
    "model.load_trained_state_dict(checkpoint[\"state_dict\"])\n",
    "model = model.to(device)\n",
    "\n",
    "### Training ### \n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Epoch:0, Training Loss:1.987: 100%|██████████| 66/66 [00:44<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.9866182804107666, 'train_average_accuracy': 0.2798751358910425, 'train_overall_accuracy': 0.3808167141500475, 'train_accuracy_class_3': 0.45698924731182794, 'train_accuracy_class_5': 0.06557377049180328, 'train_accuracy_class_4': 0.45098039215686275, 'train_accuracy_class_7': 0.48148148148148145, 'train_accuracy_class_1': 0.0, 'train_accuracy_class_2': 0.06862745098039216, 'train_accuracy_class_8': 0.9552238805970149, 'train_accuracy_class_0': 0.04, 'train_accuracy_class_6': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/338 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Validation Loss:1.831: 100%|██████████| 338/338 [00:51<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 {'validation_loss': 1.9164905548095703, 'validation_average_accuracy': 0.3576772741513186, 'validation_overall_accuracy': 0.3989253288864184, 'validation_accuracy_class_7': 0.44741235392320533, 'validation_accuracy_class_0': 0.15963060686015831, 'validation_accuracy_class_4': 0.796849087893864, 'validation_accuracy_class_5': 0.25981308411214954, 'validation_accuracy_class_1': 0.0, 'validation_accuracy_class_2': 0.0, 'validation_accuracy_class_6': 0.0, 'validation_accuracy_class_3': 0.6, 'validation_accuracy_class_8': 0.9553903345724907}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Epoch:1, Training Loss:1.482: 100%|██████████| 66/66 [00:33<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.4821525812149048, 'train_average_accuracy': 0.47181397293794775, 'train_overall_accuracy': 0.6638176638176638, 'train_accuracy_class_7': 0.8352272727272727, 'train_accuracy_class_8': 0.9552238805970149, 'train_accuracy_class_4': 0.7814569536423841, 'train_accuracy_class_3': 0.8586956521739131, 'train_accuracy_class_5': 0.2833333333333333, 'train_accuracy_class_1': 0.0, 'train_accuracy_class_2': 0.2692307692307692, 'train_accuracy_class_6': 0.0, 'train_accuracy_class_0': 0.2631578947368421}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/aradinka/miniforge3/envs/ssl/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Epoch:2, Training Loss:1.099: 100%|██████████| 66/66 [00:16<00:00,  4.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m epoch_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m metrics \u001b[39m=\u001b[39m ClasswiseAccuracy(data_config[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sample\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(sample[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39many():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aradinka/Documents/GitHub/koltiva/SSLTransformerRS/finetune_cacao/finetune_rubber.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             \u001b[39m# some s1 scenes are known to have NaNs...\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1317\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \n\u001b[1;32m   1322\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1442\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1438\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1443\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1444\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniforge3/envs/ssl/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training ### \n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(train_config['epochs']):\n",
    "    # Model Training\n",
    "    model.train()\n",
    "    step += 1\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # track performance\n",
    "    epoch_losses = torch.Tensor()\n",
    "    metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "    for idx, sample in enumerate(pbar):\n",
    "\n",
    "        if \"x\" in sample.keys():\n",
    "            if torch.isnan(sample[\"x\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "        else:\n",
    "            if torch.isnan(sample[\"s2\"]).any():\n",
    "                # some s1 scenes are known to have NaNs...\n",
    "                continue\n",
    "\n",
    "        # load input\n",
    "        s2 = sample[\"s2\"].to(device)\n",
    "        img = {\"s2\": s2}\n",
    "        \n",
    "        # if you are using a unimodal dataset (s1 for example), you may un-comment the following lines:\n",
    "        # s1 = sample[\"s1\"].to(device)\n",
    "        # img = {\"s1\": s1, \"s2\": s2}\n",
    "        \n",
    "        # load target\n",
    "        y = sample[train_config['target']].long().to(device)\n",
    "        \n",
    "        # model output\n",
    "        y_hat = model(img)\n",
    "        \n",
    "        # loss computation\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get prediction \n",
    "        _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "        epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "        metrics.add_batch(y, pred)\n",
    "\n",
    "        pbar.set_description(f\"Epoch:{epoch}, Training Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "    mean_loss = epoch_losses.mean()\n",
    "\n",
    "    train_stats = {\n",
    "            \"train_loss\": mean_loss.item(),\n",
    "            \"train_average_accuracy\": metrics.get_average_accuracy(),\n",
    "            \"train_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "            **{\n",
    "                \"train_accuracy_\" + k: v\n",
    "                for k, v in metrics.get_classwise_accuracy().items()\n",
    "            },\n",
    "        }\n",
    "    print(train_stats)\n",
    "\n",
    "    if epoch % 2 == 0:  \n",
    "\n",
    "        # Model Validation\n",
    "        model.eval()\n",
    "        pbar = tqdm(val_loader)\n",
    "\n",
    "        # track performance\n",
    "        epoch_losses = torch.Tensor()\n",
    "        metrics = ClasswiseAccuracy(data_config['num_classes'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(pbar):\n",
    "                if \"x\" in sample.keys():\n",
    "                    if torch.isnan(sample[\"x\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                else:\n",
    "                    if torch.isnan(sample[\"s2\"]).any():\n",
    "                        # some s1 scenes are known to have NaNs...\n",
    "                        continue\n",
    "                # load input\n",
    "                s2 = sample[\"s2\"].float().to(device)\n",
    "                img = {\"s2\": s2}\n",
    "\n",
    "                # if you are using a unimodal dataset (s1 for example), you may un-comment the following lines:\n",
    "                # s1 = sample[\"s1\"].to(device)\n",
    "                # img = {\"s1\": s1, \"s2\": s2}\n",
    "\n",
    "                # load target\n",
    "                y = sample[train_config['target']].long().to(device)\n",
    "\n",
    "                # model output\n",
    "                y_hat = model(img)\n",
    "\n",
    "                # loss computation\n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                # get prediction \n",
    "                _, pred = torch.max(y_hat, dim=1)\n",
    "\n",
    "                epoch_losses = torch.cat([epoch_losses, loss[None].detach().cpu()])\n",
    "                metrics.add_batch(y, pred)\n",
    "\n",
    "\n",
    "                pbar.set_description(f\"Validation Loss:{epoch_losses[-100:].mean():.4}\")\n",
    "\n",
    "            mean_loss = epoch_losses.mean()\n",
    "\n",
    "            val_stats = {\n",
    "                \"validation_loss\": mean_loss.item(),\n",
    "                \"validation_average_accuracy\": metrics.get_average_accuracy(),\n",
    "                \"validation_overall_accuracy\": metrics.get_overall_accuracy(),\n",
    "                **{\n",
    "                    \"validation_accuracy_\" + k: v\n",
    "                    for k, v in metrics.get_classwise_accuracy().items()\n",
    "                },\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch:{epoch}\", val_stats)\n",
    "            \n",
    "            # Save model checkpoint every 2 epochs \n",
    "            if epoch % 2 == 0:\n",
    "                if epoch == 0:\n",
    "                    continue\n",
    "\n",
    "                save_weights_path = (\n",
    "                    \"checkpoints/\" + \"-\".join([\"classifier\", \"epoch\", str(epoch)]) + \".pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), save_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
